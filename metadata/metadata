colbertv2:InfoSense:zw356@georgetown.edu:deep:auto::docs:8/1/2023:fullrank-yes-010:cost 12h in real time.%0AGPU%3A NVIDIA GeForce RTX 2080 Ti *1%0ACPU%3A %0AModel name%3A Intel(R) Core(TM) i9-7920X CPU @ 2.90GHz%0ACPU(s)%3A 24%0AMemory%3A%0A128GB:continue training cost 4h in real time.%0AGPU%3A NVIDIA GeForce RTX 2080 Ti *2%0ACPU%3A %0AModel name%3A Intel(R) Core(TM) i9-7920X CPU @ 2.90GHz%0ACPU(s)%3A 24%0AMemory%3A%0A128GB%0A::111:no-nnlm-no:cost 5 mins in real time.%0AGPU%3A NVIDIA GeForce RTX 2080 Ti *3%0ACPU%3A %0AModel name%3A Intel(R) Core(TM) i9-7920X CPU @ 2.90GHz%0ACPU(s)%3A 24%0AMemory%3A%0A128GB:Using Colbert-v2 and using pre-trained checkpoint and do continue training on MS MARCO v2 training data:e068c29e4d8e8cbb276da4a9606e3bc2:
cip_run_7:CIP:chenxiaoyang19@mails.ucas.ac.cn:deep:auto::passages:8/1/2023:fullrank-no-001:We use existing indexes that are currently available using the following methods%3A%0A1. Uncoil%3A msmarco-v2-passage-unicoil-0shot%0A2. doct5query%3A msmarco-v2-passage-d2q-t5%0AThus there is no need to build the index in our implementation. If the experimenter considers to build the index from scratch, the build time should be consistent with the sum of the build times of the related work above.:We use existing checkpoint files that are currently available using the following methods%3A%0A1. Uncoil%3A castorini/unicoil-msmarco-passage%0A2. MonoT5-3b%3A castorini/monot5-3b-msmarco-10k%0AThus no training is required in our implementation, and if training from scratch is required, the training time should be the same as the sum of the training times of the related work above.::110:no-prompt-no:The time required to process the test query is as follows%3A%0A1. Uncoil%3A CPU only, running time about 30 minutes%0A2. doct5query%3A CPU only, running time about 1 minute%0A3. MonoT5-3B%3A 1*RTX3090 graphics card, running time about 8 hours%0A4. GPT-3.5-turbo%3A Runtime is only related to network conditions.:Pipeline%3A%0A1. Unicoil+doct5query (pyserini)%0A2. MonoT5-3b (pygaggle)%0A3. GPT-3.5-Turbo top-50 prf reranking %0A4. GPT-3.5-Turbo top-40 reranking %0A5. GPT-3.5-Turbo top-30 reranking with different window size%0A:e9f8efe39405bb5a98a91baa1e69a739:
cip_run_1:CIP:chenxiaoyang19@mails.ucas.ac.cn:deep:auto::passages:8/1/2023:fullrank-no-001:We use existing indexes that are currently available using the following methods%3A%0A1. Uncoil%3A msmarco-v2-passage-unicoil-0shot%0A2. doct5query%3A msmarco-v2-passage-d2q-t5%0AThus there is no need to build the index in our implementation. If the experimenter considers to build the index from scratch, the build time should be consistent with the sum of the build times of the related work above.:We use existing checkpoint files that are currently available using the following methods%3A%0A1. Uncoil%3A castorini/unicoil-msmarco-passage%0A2. MonoT5-3b%3A castorini/monot5-3b-msmarco-10k%0AThus no training is required in our implementation, and if training from scratch is required, the training time should be the same as the sum of the training times of the related work above.::110:no-prompt-no:The time required to process the test query is as follows%3A%0A1. Uncoil%3A CPU only, running time about 30 minutes%0A2. doct5query%3A CPU only, running time about 1 minute%0A3. MonoT5-3B%3A 1*RTX3090 graphics card, running time about 8 hours%0A4. GPT-3.5-turbo & GPT-4%3A Runtime is only related to network conditions.:Pipeline%3A%0A1. Unicoil+doct5query (pyserini)%0A2. MonoT5-3b (pygaggle)%0A3. GPT-3.5-Turbo top-50 prf reranking %0A4. GPT-3.5-Turbo top-40 reranking %0A5. GPT-3.5-Turbo top-30 reranking with different window size%0A6. GPT-4 top-20 reranking%0A:c70237e7dcd929608deeed04828ec743:
cip_run_2:CIP:chenxiaoyang19@mails.ucas.ac.cn:deep:auto::passages:8/1/2023:fullrank-no-001:We use existing indexes that are currently available using the following methods%3A%0A1. Uncoil%3A msmarco-v2-passage-unicoil-0shot%0A2. doct5query%3A msmarco-v2-passage-d2q-t5%0AThus there is no need to build the index in our implementation. If the experimenter considers to build the index from scratch, the build time should be consistent with the sum of the build times of the related work above.:We use existing checkpoint files that are currently available using the following methods%3A%0A1. Uncoil%3A castorini/unicoil-msmarco-passage%0A2. MonoT5-3b%3A castorini/monot5-3b-msmarco-10k%0AThus no training is required in our implementation, and if training from scratch is required, the training time should be the same as the sum of the training times of the related work above.::110:no-prompt-no:The time required to process the test query is as follows%3A%0A1. Uncoil%3A CPU only, running time about 30 minutes%0A2. doct5query%3A CPU only, running time about 1 minute%0A3. MonoT5-3B%3A 1*RTX3090 graphics card, running time about 8 hours%0A4. GPT-3.5-turbo & GPT-4%3A Runtime is only related to network conditions.%0A5. Score reassignment%3A The runtime is negligible.:Pipeline%3A%0A1. Unicoil+doct5query (pyserini)%0A2. MonoT5-3b (pygaggle)%0A3. GPT-3.5-Turbo top-50 prf reranking %0A4. GPT-3.5-Turbo top-40 reranking %0A5. GPT-3.5-Turbo top-30 reranking with different window size%0A6. GPT-4 top-20 reranking%0A7. Score reassignment%0A:0c5ffb8832c3717998bd51be105fe8c8:
cip_run_3:CIP:chenxiaoyang19@mails.ucas.ac.cn:deep:auto::passages:8/1/2023:fullrank-no-001:We use existing indexes that are currently available using the following methods%3A%0A1. Uncoil%3A msmarco-v2-passage-unicoil-0shot%0A2. doct5query%3A msmarco-v2-passage-d2q-t5%0AThus there is no need to build the index in our implementation. If the experimenter considers to build the index from scratch, the build time should be consistent with the sum of the build times of the related work above.:We use existing checkpoint files that are currently available using the following methods%3A%0A1. Uncoil%3A castorini/unicoil-msmarco-passage%0A2. MonoT5-3b%3A castorini/monot5-3b-msmarco-10k%0AThus no training is required in our implementation, and if training from scratch is required, the training time should be the same as the sum of the training times of the related work above.::110:no-prompt-no:The time required to process the test query is as follows%3A%0A1. Uncoil%3A CPU only, running time about 30 minutes%0A2. doct5query%3A CPU only, running time about 1 minute%0A3. MonoT5-3B%3A 1*RTX3090 graphics card, running time about 8 hours%0A4. GPT-3.5-turbo%3A Runtime is only related to network conditions.%0A:Pipeline%3A%0A1. Unicoil+doct5query (pyserini)%0A2. MonoT5-3b (pygaggle)%0A3. GPT-3.5-Turbo top-50 prf reranking %0A4. GPT-3.5-Turbo top-40 reranking %0A5. GPT-3.5-Turbo top-30 reranking with different window size (fuse)%0A:344f93a132d35f12b38b98a10c08ac89:
cip_run_4:CIP:chenxiaoyang19@mails.ucas.ac.cn:deep:auto::passages:8/1/2023:fullrank-no-001:We use existing indexes that are currently available using the following methods%3A%0A1. Uncoil%3A msmarco-v2-passage-unicoil-0shot%0A2. doct5query%3A msmarco-v2-passage-d2q-t5%0AThus there is no need to build the index in our implementation. If the experimenter considers to build the index from scratch, the build time should be consistent with the sum of the build times of the related work above.:We use existing checkpoint files that are currently available using the following methods%3A%0A1. Uncoil%3A castorini/unicoil-msmarco-passage%0A2. MonoT5-3b%3A castorini/monot5-3b-msmarco-10k%0AThus no training is required in our implementation, and if training from scratch is required, the training time should be the same as the sum of the training times of the related work above.::110:no-prompt-no:The time required to process the test query is as follows%3A%0A1. Uncoil%3A CPU only, running time about 30 minutes%0A2. doct5query%3A CPU only, running time about 1 minute%0A3. MonoT5-3B%3A 1*RTX3090 graphics card, running time about 8 hours%0A4. GPT-3.5-turbo%3A Runtime is only related to network conditions.%0A:Pipeline%3A%0A1. Unicoil+doct5query (pyserini)%0A2. MonoT5-3b (pygaggle)%0A3. GPT-3.5-Turbo top-50 prf reranking %0A4. GPT-3.5-Turbo top-40 reranking %0A:cc884c1191cb6b6ecff0a587fb022b19:
cip_run_5:CIP:chenxiaoyang19@mails.ucas.ac.cn:deep:auto::passages:8/1/2023:fullrank-no-001:We use existing indexes that are currently available using the following methods%3A%0A1. Uncoil%3A msmarco-v2-passage-unicoil-0shot%0A2. doct5query%3A msmarco-v2-passage-d2q-t5%0AThus there is no need to build the index in our implementation. If the experimenter considers to build the index from scratch, the build time should be consistent with the sum of the build times of the related work above.:We use existing checkpoint files that are currently available using the following methods%3A%0A1. Uncoil%3A castorini/unicoil-msmarco-passage%0A2. MonoT5-3b%3A castorini/monot5-3b-msmarco-10k%0AThus no training is required in our implementation, and if training from scratch is required, the training time should be the same as the sum of the training times of the related work above.::110:no-prompt-no:The time required to process the test query is as follows%3A%0A1. Uncoil%3A CPU only, running time about 30 minutes%0A2. doct5query%3A CPU only, running time about 1 minute%0A3. MonoT5-3B%3A 1*RTX3090 graphics card, running time about 8 hours%0A4. GPT-3.5-turbo%3A Runtime is only related to network conditions.%0A:Pipeline%3A%0A1. Unicoil+doct5query (pyserini)%0A2. MonoT5-3b (pygaggle)%0A3. GPT-3.5-Turbo top-50 prf reranking %0A4. GPT-3.5-Turbo top-40 reranking %0A5. GPT-3.5-Turbo top-30 reranking with different window size%0A:2e5292385957103e82605e72ae9a9a5c:
cip_run_6:CIP:chenxiaoyang19@mails.ucas.ac.cn:deep:auto::passages:8/1/2023:fullrank-no-001:We use existing indexes that are currently available using the following methods%3A%0A1. Uncoil%3A msmarco-v2-passage-unicoil-0shot%0A2. doct5query%3A msmarco-v2-passage-d2q-t5%0AThus there is no need to build the index in our implementation. If the experimenter considers to build the index from scratch, the build time should be consistent with the sum of the build times of the related work above.:We use existing checkpoint files that are currently available using the following methods%3A%0A1. Uncoil%3A castorini/unicoil-msmarco-passage%0A2. MonoT5-3b%3A castorini/monot5-3b-msmarco-10k%0AThus no training is required in our implementation, and if training from scratch is required, the training time should be the same as the sum of the training times of the related work above.::110:no-prompt-no:The time required to process the test query is as follows%3A%0A1. Uncoil%3A CPU only, running time about 30 minutes%0A2. doct5query%3A CPU only, running time about 1 minute%0A3. MonoT5-3B%3A 1*RTX3090 graphics card, running time about 8 hours%0A4. GPT-3.5-turbo%3A Runtime is only related to network conditions.%0A:Pipeline%3A%0A1. Unicoil+doct5query (pyserini)%0A2. MonoT5-3b (pygaggle)%0A3. GPT-3.5-Turbo top-50 prf reranking %0A:21f45a56330d46b4644512a15d3a777e:
agg-cocondenser:h2oloo:s269lin@uwaterloo.ca:deep:auto::passages:8/1/2023:fullrank-yes-010:We use single A6000 to encode MS MARCO v2 passage and it takes around one day.:We train our model on single GPU for 10 hours.::010:no-nnlm-yes:We use CPU with multi-thread to perform brute-force search on Faiss Flat index.:Aggretriever is a dense retriever with semantic and lexical matching. We initialize with coCondenser and train with official MS MARCO training queries (with BM25 hard negatives) with a batch size of 64 for 3 epochs on single GPU. Detail is described in https%3A//direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00556/116046/Aggretriever-A-Simple-Approach-to-Aggregate:35fa5aa6b12b7456f99e408be96fe333:
slim-pp-0shot-uw:h2oloo:m692li@uwaterloo.ca:deep:auto::passages:8/1/2023:fullrank-yes-001:32 CPUs indexing using Pyserini in 5~10 hours:4 A6000 GPUs trained in 6 hours::010:no-nnlm-yes:36 threads CPU search using Pyserini in 11 minutes for the submission run:https%3A//arxiv.org/pdf/2302.06587.pdf:631835ca0a45a3c19d28bf12929917b7:
uot-yahoo_rankgpt35:uot-yahoo:uot-yahoo@ml.cc.tsukuba.ac.jp:deep:auto::passages:8/1/2023:rerank-yes-100:The GPU is not used in this process.:There was no fine-tuning or training of the model.::001:no-prompt-no:GPU Information%3A NVIDIA A100 80GB PCIe %0AGPU minutes%3A 120min x 1:This run utilized the GPT3.5 model to generate the re-ranking results via a List-wise approach. This is a Zero-shot learning approach.:61734f54e04fb7acb387923b2a7f968b:
uot-yahoo_rankgpt4:uot-yahoo:uot-yahoo@ml.cc.tsukuba.ac.jp:deep:auto::passages:8/1/2023:rerank-yes-100:The GPU is not used in this process.:There was no fine-tuning or training of the model.::001:no-prompt-no:GPU Information%3A NVIDIA A100 80GB PCIe %0AGPU minutes%3A 180min x 1:This run utilized the GPT4 model to generate the re-ranking results via a List-wise approach. This is a Zero-shot learning approach.:0b8e9713e24e4be4ab2d28d91ad37278:
uot-yahoo_LLMs-blender:uot-yahoo:uot-yahoo@ml.cc.tsukuba.ac.jp:deep:auto::passages:8/1/2023:rerank-yes-100:The GPU is not used in this process.:There was no fine-tuning or training of the model.::001:no-prompt-no:GPU Information%3A NVIDIA A100 80GB PCIe %0AGPU minutes%3A 2160min x 2 = 4320 GPU minutes:This run uses multiple LLM models to judge candidate document pairs in a pair-wise approach and finally aggregates the judgments of all models to generate the final ranking result. This is a Zero-shot learning approach.:061d46bce792570d42c6bfc7fca67224:
WatS-LLM-Rerank:UWaterlooMDS:dake.zhang@uwaterloo.ca:deep:auto::passages:8/1/2023:rerank-no-100:No indexing. Using the given top 100 passages for each query.:No training.::100:no-prompt-no:A100 80GB, about 8400 minutes. We didn't put effort into optimizing efficiency. This attempt is more like a proof of concept.:Prompting the llama model to assess passages.:c1861e95677e9bc9b6739ed1bf964069:
naverloo_bm25_RR:h2oloo:cadurosar@gmail.com:deep:auto::passages:8/1/2023:fullrank-no-100:Indexing cost is just the cost of indexing with BM25, we did not keep track:This year no training cost, everything is off the shelf. Training cost for the rerankers has been lost::010:no-nnlm-no:We did not keep track:First stage is BM25, second stage is an ensemble of 5 rerankers%3A%0Anaver/trecdl22-crossencoder-albert%0Anaver/trecdl22-crossencoder-debertav2%0Anaver/trecdl22-crossencoder-debertav3%0Anaver/trecdl22-crossencoder-electra%0Anaver/trecdl22-crossencoder-rankT53b-repro:55553801dd8862dffec6d75a6b6a0fe0:
naverloo-frgpt4:h2oloo:cadurosar@gmail.com:deep:auto::passages:8/1/2023:fullrank-no-111:We did not keep track:This year no training cost, everything is off the shelf. Training cost for the rerankers and SPLADE has been lost::010:no-prompt-no:We did not keep track:First stage is a very large ensemble of BM25+DOCT5+SPLADEPP+SPLADESD+AGG+SLIM, second stage is an ensemble of 5 rerankers%3A%0Anaver/trecdl22-crossencoder-albert%0Anaver/trecdl22-crossencoder-debertav2%0Anaver/trecdl22-crossencoder-debertav3%0Anaver/trecdl22-crossencoder-electra%0Anaver/trecdl22-crossencoder-rankT53b-repro%0AWe then do a third step with an ensemble of 3 duo rankers over the top50, duoT5, PRP-FlanT5-3b and PRP-FlanT5-UL2. We finally finish by applying RankGPT4 over the top30 and ensembling with the previous step.:4f04190938971d286d1edf8f09c215d2:
naverloo-rgpt4:h2oloo:cadurosar@gmail.com:deep:auto::passages:8/1/2023:fullrank-no-111:We did not keep track:This year no training cost, everything is off the shelf. Training cost for the rerankers and SPLADE has been lost::010:no-prompt-yes:We did not keep track:First stage is a very large ensemble of BM25+DOCT5+SPLADEPP+SPLADESD+AGG+SLIM, second stage is an ensemble of 5 rerankers%3A%0Anaver/trecdl22-crossencoder-albert%0Anaver/trecdl22-crossencoder-debertav2%0Anaver/trecdl22-crossencoder-debertav3%0Anaver/trecdl22-crossencoder-electra%0Anaver/trecdl22-crossencoder-rankT53b-repro%0AWe then do a third step with an ensemble of 3 duo rankers over the top50, duoT5, PRP-FlanT5-3b and PRP-FlanT5-UL2. We finally finish by applying RankGPT4 over the top30.:ae6ba03d1aaf61f3aeecb3b4d37a510f:
splade_pp_ensemble_distil:h2oloo:cadurosar@gmail.com:deep:auto::passages:8/1/2023:fullrank-yes-001:We did not keep track:We did not keep track (available off the shelf)::010:no-nnlm-yes:We did not keep track:Splade++ Ensemble distil available on huggingface:0cbab324dbfebca7faf233901c9a92d9:
splade_pp_self_distil:h2oloo:cadurosar@gmail.com:deep:auto::passages:8/1/2023:fullrank-yes-001:We did not keep track:We did not keep track (available off the shelf)::010:no-nnlm-yes:We did not keep track:Splade++ Self distil available on huggingface:50f8933fbff1008c97db3ef69493fa4b:
bm25_splades:h2oloo:cadurosar@gmail.com:deep:auto::passages:8/1/2023:fullrank-yes-101:We did not keep track:We did not keep track (available off the shelf)::010:no-nnlm-yes:We did not keep track:Ensemble of BM25 + SPLADE++SD + SPLADE++ED:26123ed5ab46fe0d76f59f80fb5a3474:
naverloo_fs:h2oloo:cadurosar@gmail.com:deep:auto::passages:8/1/2023:fullrank-yes-111:We did not keep track:We did not keep track (available off the shelf)::010:no-nnlm-yes:We did not keep track:Ensemble of BM25 + SPLADE++SD + SPLADE++ED + SLIM + AGGRetriever:1331c0c525a626c61a7df6a41c4db51d:
naverloo_fs_RR:h2oloo:cadurosar@gmail.com:deep:auto::passages:8/1/2023:fullrank-no-111:We did not keep track:We did not keep track (available off the shelf)::010:no-nnlm-yes:We did not keep track:First stage is an ensemble of BM25 + SPLADE++SD + SPLADE++ED + SLIM + AGGRetriever. %0ASecond stage is an ensemble of 5 rerankers%3A%0Anaver/trecdl22-crossencoder-albert%0Anaver/trecdl22-crossencoder-debertav2%0Anaver/trecdl22-crossencoder-debertav3%0Anaver/trecdl22-crossencoder-electra%0Anaver/trecdl22-crossencoder-rankT53b-repro:bda95ea90ed615d1fcc5c47a29b105f6:
naverloo_fs_RR_duo:h2oloo:cadurosar@gmail.com:deep:auto::passages:8/1/2023:fullrank-no-111:We did not keep track:We did not keep track (available off the shelf)::010:no-prompt-yes:We did not keep track:First stage is an ensemble of BM25 + SPLADE++SD + SPLADE++ED + SLIM + AGGRetriever. %0ASecond stage is an ensemble of 5 rerankers%3A%0Anaver/trecdl22-crossencoder-albert%0Anaver/trecdl22-crossencoder-debertav2%0Anaver/trecdl22-crossencoder-debertav3%0Anaver/trecdl22-crossencoder-electra%0Anaver/trecdl22-crossencoder-rankT53b-repro%0AThird step is an ensemble of 3 duo rankers over the top50%3A%0AduoT5 %0APRP-FlanT5-3b%0APRP-FlanT5-UL2:299dcb3e07daf943a746119917a2bc29:
naverloo_bm25_splades_RR:h2oloo:cadurosar@gmail.com:deep:auto::passages:8/1/2023:fullrank-no-111:We did not keep track:We did not keep track (available off the shelf)::010:no-nnlm-no:We did not keep track:First stage is an ensemble of BM25 + SPLADE++SD + SPLADE++ED%0ASecond stage is an ensemble of 5 rerankers%3A%0Anaver/trecdl22-crossencoder-albert%0Anaver/trecdl22-crossencoder-debertav2%0Anaver/trecdl22-crossencoder-debertav3%0Anaver/trecdl22-crossencoder-electra%0Anaver/trecdl22-crossencoder-rankT53b-repro:64c59aa9bba0142857376ef7ff03d3e6:
D_bm25_splades:h2oloo:cadurosar@gmail.com:deep:auto::docs:8/1/2023:fullrank-yes-101:We did not keep track:We did not keep track (available off the shelf)::010:no-nnlm-yes:We did not keep track:Ensemble of BM25 + SPLADE++SD + SPLADE++ED%0A:31d0e3e09e1326ecac424b2c29574cdf:
D_naverloo-frgpt4:h2oloo:cadurosar@gmail.com:deep:auto::docs:8/1/2023:fullrank-no-111:We did not keep track:We did not keep track (available off the shelf)::010:no-prompt-no:We did not keep track:First stage is an ensemble of BM25 + SPLADE++SD + SPLADE++ED + SLIM + AGGRetriever%0ASecond stage is an ensemble of 5 rerankers%3A%0Anaver/trecdl22-crossencoder-albert%0Anaver/trecdl22-crossencoder-debertav2%0Anaver/trecdl22-crossencoder-debertav3%0Anaver/trecdl22-crossencoder-electra%0Anaver/trecdl22-crossencoder-rankT53b-repro%0AThird step is an ensemble of 3 duo rankers over the top50%0AduoT5%0APRP-FlanT5-3b%0APRP-FlanT5-UL2%0AFourth step is RankGPT4 over the top30, which is then ensemble with the 3rd stage%0A:3290f826729d8e377e4a0bf89b99fa99:
D_naverloo_bm25_RR:h2oloo:cadurosar@gmail.com:deep:auto::docs:8/1/2023:fullrank-no-100:We did not keep track:We did not keep track (available off the shelf)::010:no-nnlm-no:We did not keep track:First stage is BM25, second stage is an ensemble of 5 rerankers%3A%0Anaver/trecdl22-crossencoder-albert%0Anaver/trecdl22-crossencoder-debertav2%0Anaver/trecdl22-crossencoder-debertav3%0Anaver/trecdl22-crossencoder-electra%0Anaver/trecdl22-crossencoder-rankT53b-repro%0A:dcd5a5ef98526d1d0627b02380587262:
D_naverloo_bm_splade_RR:h2oloo:cadurosar@gmail.com:deep:auto::docs:8/1/2023:fullrank-no-101:We did not keep track:We did not keep track (available off the shelf)::010:no-nnlm-no:We did not keep track:First stage is BM25+SPLADE++ED+SPLADE++SD, second stage is an ensemble of 5 rerankers%3A%0Anaver/trecdl22-crossencoder-albert%0Anaver/trecdl22-crossencoder-debertav2%0Anaver/trecdl22-crossencoder-debertav3%0Anaver/trecdl22-crossencoder-electra%0Anaver/trecdl22-crossencoder-rankT53b-repro%0A:8fd7ab922d5206f3bc095a6de51c15aa:
WatS-Augmented-BM25:UWaterlooMDS:dake.zhang@uwaterloo.ca:deep:auto::passages:8/1/2023:fullrank-yes-001:Pyserini indexing the corpus.:No training.::100:no-prompt-no:A100 80G, about 840 minutes. The code was not optimized for efficiency. This run is for proof of concept.:Prompting a LLM to rewrite queries.:6180f09edd151fb24f0140fd6fb94dde:
uogtr_dph:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-yes-100:Lexical indexing of msmarco-passage-v2; using CPU:None::000:no-trad-yes:DPH; using CPU:Performs DPH on the entire msmarco-passage-v2 inverted index.:6916de76e58cac8efa85b3a61f9dce83:
uogtr_dph_bo1:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-no-100:Lexical indexing of msmarco-passage-v2; using CPU:None::000:no-trad-yes:DPH; Bo1 expansion; DPH; using CPU:Performs DPH with Bo1 query expansion on the entire msmarco-passage-v2 inverted index.:3d392481e3127b27969eb7265c4b8cf9:
uogtr_be:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-no-100:Lexical indexing of msmarco-passage-v2; using CPU:We use publicly available trained models. No new training costs were made for this submission.::010:no-nnlm-yes:BM25; ELECTRA re-ranking; using CPU and GPU:BM25 retrieval, re-ranked using crystina-z/monoELECTRA_LCE_nneg31:4dfa2a0d7956dd5cc70146b330827355:
uogtr_se:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-no-001:SPLADE inference and indexing of msmarco-passage-v2; using CPU and GPU:We use publicly available trained models. No new training costs were made for this submission.::010:no-nnlm-yes:SPLADE query encoding; TF; ELECTRA re-ranking; using CPU and GPU:SPLADE retrieval using naver/splade-cocondenser-ensembledistil, re-ranked using crystina-z/monoELECTRA_LCE_nneg31:6a3200e762c38ab4b5909c809bcb65a0:
uogtr_s:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-no-001:SPLADE inference and indexing of msmarco-passage-v2; using CPU and GPU:We use publicly available trained models. No new training costs were made for this submission.::010:no-nnlm-yes:SPLADE query encoding; TF; using CPU and GPU:SPLADE retrieval using naver/splade-cocondenser-ensembledistil:23aec2a3cac327addba6f93b63f4ba6e:
uogtr_se_gb:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-no-001:SPLADE inference and indexing of msmarco-passage-v2,  NN Corpus graph ; using CPU and GPU:We use publicly available trained models. No new training costs were made for this submission.::010:no-nnlm-no:SPLADE query encoding; TF, BM25 Graph adaptive rerank, ELECTRA re-ranking; using CPU and GPU:SPLADE retrieval using naver/splade-cocondenser-ensembledistil,  adaptive reranking using crystina-z/monoELECTRA_LCE_nneg31 with BM25 Graph:9710da3e2e4a1e5da0664b8c4cd7c915:
uogtr_be_gb:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-no-100:Lexical indexing of msmarco-passage-v2,  NN Corpus graph; using CPU:We use publicly available trained models. No new training costs were made for this submission.::010:no-nnlm-no:BM25, BM25 Graph adaptive rerank, ELECTRA reranking; Using GPU and CPU:BM25 over entire msmarco-passage-v2 inverted index,adaptive reranking using crystina-z/monoELECTRA_LCE_nneg31 with BM25 Graph:d200f64351d1efac7c775e4307c0279a:
uogtr_qr_be_gb:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-no-100:Lexical indexing of msmarco-passage-v2,  Nearest neighbour by BM25 graph; using CPU:We use publicly available trained models. No new training costs were made for this submission.::010:no-prompt-no:FLANT5-XXL Generative Query Reformulation, BM25 Graph adaptive rerank,ELECTRA reranking; Using GPU and CPU:Generative query expansion using google/flant5-xxl (8-bit quantized), BM25 over entire msmarco-passage-v2 inverted index, adaptive reranking using crystina-z/monoELECTRA_LCE_nneg31 with BM25 Graph:8d457d9392e936e00dc7187a9232a277:
uogtr_b_grf_e:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-no-100:Lexical indexing of msmarco-passage-v2; using CPU:We use publicly available trained models. No new training costs were made for this submission.::010:no-prompt-no:BM25, FLANT5-XXL Generative Relevance Feedback, ELECTRA reranking; Using CPU and GPU:BM25 over entire msmarco-passage-v2 inverted index, Generative relevance feedback using google/flant5-xxl (8 bit quantized), reranking using crystina-z/monoELECTRA_LCE_nneg31 :01ec324fe73dc072c726429e7859936c:
uogtr_qr_be:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-no-100:Lexical indexing of msmarco-passage-v2; using CPU:We use publicly available trained models. No new training costs were made for this submission.::010:no-prompt-no:FLANT5-XXL Generative Query Reformulation, BM25, ELECTRA reranking; Using GPU and CPU:Generative query expansion using google/flant5-xxl (8-bit quantized), BM25 over entire msmarco-passage-v2 inverted index, reranking using crystina-z/monoELECTRA_LCE_nneg31:f9267814b5b855fc628abdb26a76c708:
uogtr_b_grf_e_gb:uogTr:a.parry.1@research.gla.ac.uk:deep:auto::passages:8/1/2023:fullrank-no-100:Lexical indexing of msmarco-passage-v2,  Nearest neighbour by BM25 graph; using CPU:We use publicly available trained models. No new training costs were made for this submission.::010:no-prompt-no:FLANT5-XXL Generative Query Reformulation, BM25 Graph adaptive rerank,ELECTRA reranking; Using GPU and CPU:Generative query expansion using google/flant5-xxl (8-bit quantized), BM25 over entire msmarco-passage-v2 inverted index, adaptive reranking using crystina-z/monoELECTRA_LCE_nneg31 with BM25 Graph:9b16c2da7d2198c3c8a3c7afdc7b665d:
